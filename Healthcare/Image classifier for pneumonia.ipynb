{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 1.14.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "#pre trained inception model\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "#image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "print(\"Tensorflow version \" + tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count number of files in folder and subfolders\n",
    "def get_nb_files (path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    return sum([len(files) for r, d, files in os.walk(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of subfolders, which in this case are the clases.\n",
    "def get_nb_subfolders(path):\n",
    "    if not os.path.exists(path):\n",
    "        return 0\n",
    "    return sum([len(d) for r, d, files in os.walk(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define image generators:\n",
    "def create_img_generator():\n",
    "    return ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Main\n",
    "Image_width, Image_height = 299, 299\n",
    "Training_Epochs = 2\n",
    "Batch_size = 32 #number of images in batch\n",
    "Number_FC_Neurons = 1024\n",
    "\n",
    "train_dir = './data/train'\n",
    "validate_dir = './data/val' \n",
    "\n",
    "num_trian_samples = get_nb_files(train_dir)\n",
    "num_classes = get_nb_subfolders(train_dir)\n",
    "num_validate_samples = get_nb_files(validate_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "### Define data preprocessing ###\n",
    "\n",
    "#Define image generators for training and testing\n",
    "train_image_gen = create_img_generator()\n",
    "test_image_gen = create_img_generator()\n",
    "\n",
    "#Training image generator\n",
    "train_generator = train_image_gen.flow_from_directory(\n",
    "    train_dir, target_size=(Image_width,Image_height), batch_size=Batch_size,\n",
    "    seed = 42)\n",
    "\n",
    "#Validation image generator(Define the generator for the validation folder)\n",
    "validation_generator = test_image_gen.flow_from_directory(\n",
    "    validate_dir,\n",
    "    target_size=(Image_width,Image_height),\n",
    "    batch_size=Batch_size,\n",
    "    seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Viridiana\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Viridiana\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Viridiana\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Viridiana\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\Viridiana\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "87916544/87910968 [==============================] - ETA: 10 - ETA: 5: - ETA: 5: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 58 - ETA: 56 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 51 - ETA: 51 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 39 - ETA: 41 - ETA: 42 - ETA: 43 - ETA: 42 - ETA: 44 - ETA: 41 - ETA: 40 - ETA: 41 - ETA: 40 - ETA: 39 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 34s 0us/step\n",
      "Inception V3 base model without last FC loaded\n"
     ]
    }
   ],
   "source": [
    "#Load the inception v3 model\n",
    "InceptionV3_base_model = InceptionV3 (weights = 'imagenet', include_top = False)\n",
    "print('Inception V3 base model without last FC loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the layers in the nes classification prediction\n",
    "x = InceptionV3_base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(Number_FC_Neurons, activation='relu')(x) #new FC layer\n",
    "predictions = Dense(num_classes,activation = 'softmax')(x) #new softmax layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define new model containing the first layers from de inception v3 base and the new classifier\n",
    "model = Model(inputs = InceptionV3_base_model.input, outputs = predictions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 3, None, None 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, None, Non 864         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, None, Non 96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 32, None, Non 0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, None, Non 9216        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, None, Non 96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 32, None, Non 0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 64, None, Non 18432       activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64, None, Non 192         conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 64, None, Non 0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 64, None, Non 0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 80, None, Non 5120        max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 80, None, Non 240         conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 80, None, Non 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 192, None, No 138240      activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 192, None, No 576         conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 192, None, No 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 192, None, No 0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 64, None, Non 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64, None, Non 192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 64, None, Non 0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 48, None, Non 9216        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 96, None, Non 55296       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 48, None, Non 144         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 96, None, Non 288         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 48, None, Non 0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 96, None, Non 0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_1 (AveragePoo (None, 192, None, No 0           max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 64, None, Non 12288       max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 64, None, Non 76800       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 96, None, Non 82944       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, None, Non 6144        average_pooling2d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64, None, Non 192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 64, None, Non 192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 96, None, Non 288         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, None, Non 96          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 64, None, Non 0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 64, None, Non 0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 96, None, Non 0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 32, None, Non 0           batch_normalization_12[0][0]     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 256, None, No 0           activation_6[0][0]               \n",
      "                                                                 activation_8[0][0]               \n",
      "                                                                 activation_11[0][0]              \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 64, None, Non 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 64, None, Non 192         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 64, None, Non 0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 48, None, Non 12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 96, None, Non 55296       activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 48, None, Non 144         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 96, None, Non 288         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 48, None, Non 0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 96, None, Non 0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_2 (AveragePoo (None, 256, None, No 0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 64, None, Non 16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 64, None, Non 76800       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 96, None, Non 82944       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 64, None, Non 16384       average_pooling2d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 64, None, Non 192         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 64, None, Non 192         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 96, None, Non 288         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 64, None, Non 192         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 64, None, Non 0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 64, None, Non 0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 96, None, Non 0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 64, None, Non 0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 288, None, No 0           activation_13[0][0]              \n",
      "                                                                 activation_15[0][0]              \n",
      "                                                                 activation_18[0][0]              \n",
      "                                                                 activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 64, None, Non 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 64, None, Non 192         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 64, None, Non 0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 48, None, Non 13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 96, None, Non 55296       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 48, None, Non 144         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 96, None, Non 288         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 48, None, Non 0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 96, None, Non 0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_3 (AveragePoo (None, 288, None, No 0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 64, None, Non 18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 64, None, Non 76800       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 96, None, Non 82944       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 64, None, Non 18432       average_pooling2d_3[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 64, None, Non 192         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 64, None, Non 192         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 96, None, Non 288         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 64, None, Non 192         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 64, None, Non 0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 64, None, Non 0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 96, None, Non 0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 64, None, Non 0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 288, None, No 0           activation_20[0][0]              \n",
      "                                                                 activation_22[0][0]              \n",
      "                                                                 activation_25[0][0]              \n",
      "                                                                 activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 64, None, Non 18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 64, None, Non 192         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 64, None, Non 0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 96, None, Non 55296       activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_29 (BatchNo (None, 96, None, Non 288         conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 96, None, Non 0           batch_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 384, None, No 995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 96, None, Non 82944       activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 384, None, No 1152        conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_30 (BatchNo (None, 96, None, Non 288         conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 384, None, No 0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 96, None, Non 0           batch_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 288, None, No 0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 768, None, No 0           activation_27[0][0]              \n",
      "                                                                 activation_30[0][0]              \n",
      "                                                                 max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 128, None, No 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_35 (BatchNo (None, 128, None, No 384         conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 128, None, No 0           batch_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 128, None, No 114688      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_36 (BatchNo (None, 128, None, No 384         conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 128, None, No 0           batch_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 128, None, No 98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 128, None, No 114688      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_32 (BatchNo (None, 128, None, No 384         conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_37 (BatchNo (None, 128, None, No 384         conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 128, None, No 0           batch_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 128, None, No 0           batch_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 128, None, No 114688      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 128, None, No 114688      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_33 (BatchNo (None, 128, None, No 384         conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_38 (BatchNo (None, 128, None, No 384         conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 128, None, No 0           batch_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 128, None, No 0           batch_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_4 (AveragePoo (None, 768, None, No 0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 192, None, No 147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 192, None, No 172032      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 192, None, No 172032      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 192, None, No 147456      average_pooling2d_4[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_31 (BatchNo (None, 192, None, No 576         conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_34 (BatchNo (None, 192, None, No 576         conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_39 (BatchNo (None, 192, None, No 576         conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_40 (BatchNo (None, 192, None, No 576         conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 192, None, No 0           batch_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 192, None, No 0           batch_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 192, None, No 0           batch_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 192, None, No 0           batch_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 768, None, No 0           activation_31[0][0]              \n",
      "                                                                 activation_34[0][0]              \n",
      "                                                                 activation_39[0][0]              \n",
      "                                                                 activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 160, None, No 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_45 (BatchNo (None, 160, None, No 480         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 160, None, No 0           batch_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 160, None, No 179200      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_46 (BatchNo (None, 160, None, No 480         conv2d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 160, None, No 0           batch_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 160, None, No 122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 160, None, No 179200      activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_42 (BatchNo (None, 160, None, No 480         conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_47 (BatchNo (None, 160, None, No 480         conv2d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 160, None, No 0           batch_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 160, None, No 0           batch_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 160, None, No 179200      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 160, None, No 179200      activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_43 (BatchNo (None, 160, None, No 480         conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_48 (BatchNo (None, 160, None, No 480         conv2d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 160, None, No 0           batch_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 160, None, No 0           batch_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_5 (AveragePoo (None, 768, None, No 0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 192, None, No 147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 192, None, No 215040      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 192, None, No 215040      activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 192, None, No 147456      average_pooling2d_5[0][0]        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "batch_normalization_41 (BatchNo (None, 192, None, No 576         conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_44 (BatchNo (None, 192, None, No 576         conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_49 (BatchNo (None, 192, None, No 576         conv2d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 192, None, No 576         conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 192, None, No 0           batch_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 192, None, No 0           batch_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 192, None, No 0           batch_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 192, None, No 0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 768, None, No 0           activation_41[0][0]              \n",
      "                                                                 activation_44[0][0]              \n",
      "                                                                 activation_49[0][0]              \n",
      "                                                                 activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 160, None, No 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 160, None, No 480         conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_55 (Activation)      (None, 160, None, No 0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 160, None, No 179200      activation_55[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 160, None, No 480         conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_56 (Activation)      (None, 160, None, No 0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 160, None, No 122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 160, None, No 179200      activation_56[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 160, None, No 480         conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 160, None, No 480         conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 160, None, No 0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_57 (Activation)      (None, 160, None, No 0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 160, None, No 179200      activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 160, None, No 179200      activation_57[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 160, None, No 480         conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 160, None, No 480         conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 160, None, No 0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_58 (Activation)      (None, 160, None, No 0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 768, None, No 0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 192, None, No 147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 192, None, No 215040      activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 192, None, No 215040      activation_58[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 192, None, No 147456      average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 192, None, No 576         conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 192, None, No 576         conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 192, None, No 576         conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 192, None, No 576         conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 192, None, No 0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 192, None, No 0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_59 (Activation)      (None, 192, None, No 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_60 (Activation)      (None, 192, None, No 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 768, None, No 0           activation_51[0][0]              \n",
      "                                                                 activation_54[0][0]              \n",
      "                                                                 activation_59[0][0]              \n",
      "                                                                 activation_60[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 192, None, No 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 192, None, No 576         conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_65 (Activation)      (None, 192, None, No 0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 192, None, No 258048      activation_65[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 192, None, No 576         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_66 (Activation)      (None, 192, None, No 0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 192, None, No 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 192, None, No 258048      activation_66[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 192, None, No 576         conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 192, None, No 576         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_62 (Activation)      (None, 192, None, No 0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_67 (Activation)      (None, 192, None, No 0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 192, None, No 258048      activation_62[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 192, None, No 258048      activation_67[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 192, None, No 576         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 192, None, No 576         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_63 (Activation)      (None, 192, None, No 0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_68 (Activation)      (None, 192, None, No 0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_7 (AveragePoo (None, 768, None, No 0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 192, None, No 147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 192, None, No 258048      activation_63[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 192, None, No 258048      activation_68[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 192, None, No 147456      average_pooling2d_7[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 192, None, No 576         conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 192, None, No 576         conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 192, None, No 576         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 192, None, No 576         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_61 (Activation)      (None, 192, None, No 0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_64 (Activation)      (None, 192, None, No 0           batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_69 (Activation)      (None, 192, None, No 0           batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_70 (Activation)      (None, 192, None, No 0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 768, None, No 0           activation_61[0][0]              \n",
      "                                                                 activation_64[0][0]              \n",
      "                                                                 activation_69[0][0]              \n",
      "                                                                 activation_70[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 192, None, No 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 192, None, No 576         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_73 (Activation)      (None, 192, None, No 0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 192, None, No 258048      activation_73[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_normalization_74 (BatchNo (None, 192, None, No 576         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_74 (Activation)      (None, 192, None, No 0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 192, None, No 147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_75 (Conv2D)              (None, 192, None, No 258048      activation_74[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 192, None, No 576         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_75 (BatchNo (None, 192, None, No 576         conv2d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_71 (Activation)      (None, 192, None, No 0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_75 (Activation)      (None, 192, None, No 0           batch_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 320, None, No 552960      activation_71[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 192, None, No 331776      activation_75[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 320, None, No 960         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 192, None, No 576         conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_72 (Activation)      (None, 320, None, No 0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_76 (Activation)      (None, 192, None, No 0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 768, None, No 0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 1280, None, N 0           activation_72[0][0]              \n",
      "                                                                 activation_76[0][0]              \n",
      "                                                                 max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 448, None, No 573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 448, None, No 1344        conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_81 (Activation)      (None, 448, None, No 0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 384, None, No 491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 384, None, No 1548288     activation_81[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 384, None, No 1152        conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 384, None, No 1152        conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_78 (Activation)      (None, 384, None, No 0           batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_82 (Activation)      (None, 384, None, No 0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 384, None, No 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 384, None, No 442368      activation_78[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 384, None, No 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 384, None, No 442368      activation_82[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_8 (AveragePoo (None, 1280, None, N 0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 320, None, No 409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 384, None, No 1152        conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 384, None, No 1152        conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 384, None, No 1152        conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 384, None, No 1152        conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 192, None, No 245760      average_pooling2d_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 320, None, No 960         conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_79 (Activation)      (None, 384, None, No 0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_80 (Activation)      (None, 384, None, No 0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_83 (Activation)      (None, 384, None, No 0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "activation_84 (Activation)      (None, 384, None, No 0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 192, None, No 576         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_77 (Activation)      (None, 320, None, No 0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 768, None, No 0           activation_79[0][0]              \n",
      "                                                                 activation_80[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 768, None, No 0           activation_83[0][0]              \n",
      "                                                                 activation_84[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_85 (Activation)      (None, 192, None, No 0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 2048, None, N 0           activation_77[0][0]              \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_1[0][0]              \n",
      "                                                                 activation_85[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 448, None, No 917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 448, None, No 1344        conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_90 (Activation)      (None, 448, None, No 0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 384, None, No 786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 384, None, No 1548288     activation_90[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 384, None, No 1152        conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 384, None, No 1152        conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_87 (Activation)      (None, 384, None, No 0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_91 (Activation)      (None, 384, None, No 0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 384, None, No 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 384, None, No 442368      activation_87[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 384, None, No 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 384, None, No 442368      activation_91[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 2048, None, N 0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 320, None, No 655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 384, None, No 1152        conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 384, None, No 1152        conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 384, None, No 1152        conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 384, None, No 1152        conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 192, None, No 393216      average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 320, None, No 960         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_88 (Activation)      (None, 384, None, No 0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_89 (Activation)      (None, 384, None, No 0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_92 (Activation)      (None, 384, None, No 0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_93 (Activation)      (None, 384, None, No 0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 192, None, No 576         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_86 (Activation)      (None, 320, None, No 0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 768, None, No 0           activation_88[0][0]              \n",
      "                                                                 activation_89[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 768, None, No 0           activation_92[0][0]              \n",
      "                                                                 activation_93[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 192, None, No 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 2048, None, N 0           activation_86[0][0]              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 2)            2050        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,903,010\n",
      "Trainable params: 23,868,578\n",
      "Non-trainable params: 34,432\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Print model structure diagram\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transfer learning\n",
    "for layer in InceptionV3_base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "#Define model compile for transfer learning\n",
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 185/5219 [>.............................] - ETA: 26:03:11 - loss: 0.3930 - acc: 0.750 - ETA: 24:46:13 - loss: 0.4057 - acc: 0.781 - ETA: 24:56:11 - loss: 0.4234 - acc: 0.802 - ETA: 25:45:35 - loss: 0.4166 - acc: 0.828 - ETA: 25:46:42 - loss: 0.4013 - acc: 0.843 - ETA: 25:32:46 - loss: 0.4029 - acc: 0.822 - ETA: 25:24:54 - loss: 0.3993 - acc: 0.821 - ETA: 25:17:06 - loss: 0.4055 - acc: 0.820 - ETA: 25:18:03 - loss: 0.3922 - acc: 0.822 - ETA: 25:14:43 - loss: 0.3908 - acc: 0.821 - ETA: 25:25:28 - loss: 0.3911 - acc: 0.818 - ETA: 25:40:41 - loss: 0.3886 - acc: 0.817 - ETA: 25:35:27 - loss: 0.3897 - acc: 0.817 - ETA: 25:31:06 - loss: 0.3895 - acc: 0.812 - ETA: 25:27:16 - loss: 0.3909 - acc: 0.812 - ETA: 25:23:51 - loss: 0.3856 - acc: 0.816 - ETA: 25:19:53 - loss: 0.3795 - acc: 0.823 - ETA: 25:22:04 - loss: 0.3869 - acc: 0.810 - ETA: 25:19:50 - loss: 0.3806 - acc: 0.815 - ETA: 25:25:22 - loss: 0.3731 - acc: 0.821 - ETA: 25:22:44 - loss: 0.3703 - acc: 0.825 - ETA: 25:21:21 - loss: 0.3663 - acc: 0.829 - ETA: 25:22:55 - loss: 0.3664 - acc: 0.831 - ETA: 25:23:05 - loss: 0.3611 - acc: 0.835 - ETA: 25:20:17 - loss: 0.3661 - acc: 0.831 - ETA: 25:18:07 - loss: 0.3593 - acc: 0.836 - ETA: 25:16:18 - loss: 0.3568 - acc: 0.838 - ETA: 25:18:35 - loss: 0.3552 - acc: 0.838 - ETA: 25:22:44 - loss: 0.3578 - acc: 0.837 - ETA: 25:18:26 - loss: 0.3576 - acc: 0.837 - ETA: 25:17:30 - loss: 0.3565 - acc: 0.837 - ETA: 25:11:23 - loss: 0.3536 - acc: 0.839 - ETA: 25:05:11 - loss: 0.3496 - acc: 0.840 - ETA: 25:01:50 - loss: 0.3484 - acc: 0.841 - ETA: 25:02:19 - loss: 0.3426 - acc: 0.842 - ETA: 25:09:00 - loss: 0.3434 - acc: 0.841 - ETA: 25:06:09 - loss: 0.3506 - acc: 0.836 - ETA: 25:01:03 - loss: 0.3463 - acc: 0.838 - ETA: 24:56:43 - loss: 0.3442 - acc: 0.841 - ETA: 24:59:02 - loss: 0.3447 - acc: 0.842 - ETA: 24:58:58 - loss: 0.3490 - acc: 0.841 - ETA: 24:55:18 - loss: 0.3464 - acc: 0.843 - ETA: 24:51:07 - loss: 0.3450 - acc: 0.844 - ETA: 24:46:58 - loss: 0.3438 - acc: 0.843 - ETA: 24:42:32 - loss: 0.3461 - acc: 0.842 - ETA: 24:38:17 - loss: 0.3428 - acc: 0.844 - ETA: 24:34:13 - loss: 0.3420 - acc: 0.843 - ETA: 24:30:23 - loss: 0.3412 - acc: 0.843 - ETA: 24:26:32 - loss: 0.3398 - acc: 0.845 - ETA: 24:23:01 - loss: 0.3401 - acc: 0.845 - ETA: 24:20:04 - loss: 0.3393 - acc: 0.845 - ETA: 24:16:45 - loss: 0.3395 - acc: 0.845 - ETA: 24:13:26 - loss: 0.3396 - acc: 0.844 - ETA: 24:10:29 - loss: 0.3412 - acc: 0.844 - ETA: 24:07:16 - loss: 0.3383 - acc: 0.846 - ETA: 24:04:05 - loss: 0.3368 - acc: 0.846 - ETA: 24:00:57 - loss: 0.3377 - acc: 0.845 - ETA: 23:58:23 - loss: 0.3362 - acc: 0.847 - ETA: 23:57:02 - loss: 0.3389 - acc: 0.845 - ETA: 23:55:05 - loss: 0.3362 - acc: 0.846 - ETA: 23:52:42 - loss: 0.3330 - acc: 0.848 - ETA: 23:49:42 - loss: 0.3314 - acc: 0.849 - ETA: 23:47:08 - loss: 0.3294 - acc: 0.850 - ETA: 23:46:04 - loss: 0.3318 - acc: 0.850 - ETA: 23:42:41 - loss: 0.3300 - acc: 0.851 - ETA: 23:42:42 - loss: 0.3274 - acc: 0.853 - ETA: 23:42:16 - loss: 0.3250 - acc: 0.854 - ETA: 23:41:38 - loss: 0.3262 - acc: 0.852 - ETA: 23:41:45 - loss: 0.3236 - acc: 0.854 - ETA: 23:39:29 - loss: 0.3209 - acc: 0.855 - ETA: 23:36:39 - loss: 0.3212 - acc: 0.856 - ETA: 23:38:33 - loss: 0.3188 - acc: 0.857 - ETA: 23:38:16 - loss: 0.3179 - acc: 0.858 - ETA: 23:37:08 - loss: 0.3178 - acc: 0.859 - ETA: 23:36:28 - loss: 0.3174 - acc: 0.860 - ETA: 23:36:06 - loss: 0.3163 - acc: 0.860 - ETA: 23:38:53 - loss: 0.3158 - acc: 0.860 - ETA: 23:37:04 - loss: 0.3146 - acc: 0.860 - ETA: 23:38:23 - loss: 0.3121 - acc: 0.861 - ETA: 23:36:40 - loss: 0.3111 - acc: 0.862 - ETA: 23:35:42 - loss: 0.3106 - acc: 0.862 - ETA: 23:33:41 - loss: 0.3102 - acc: 0.863 - ETA: 23:31:45 - loss: 0.3102 - acc: 0.863 - ETA: 23:29:59 - loss: 0.3092 - acc: 0.863 - ETA: 23:27:55 - loss: 0.3106 - acc: 0.863 - ETA: 23:25:25 - loss: 0.3091 - acc: 0.864 - ETA: 23:22:35 - loss: 0.3091 - acc: 0.865 - ETA: 23:18:22 - loss: 0.3098 - acc: 0.865 - ETA: 23:14:19 - loss: 0.3079 - acc: 0.866 - ETA: 23:10:23 - loss: 0.3067 - acc: 0.867 - ETA: 23:06:12 - loss: 0.3054 - acc: 0.868 - ETA: 23:02:06 - loss: 0.3036 - acc: 0.869 - ETA: 22:58:02 - loss: 0.3014 - acc: 0.870 - ETA: 22:54:09 - loss: 0.2995 - acc: 0.871 - ETA: 22:50:19 - loss: 0.2987 - acc: 0.872 - ETA: 22:46:38 - loss: 0.2973 - acc: 0.872 - ETA: 22:42:56 - loss: 0.2964 - acc: 0.873 - ETA: 22:39:21 - loss: 0.2956 - acc: 0.873 - ETA: 22:35:50 - loss: 0.2939 - acc: 0.873 - ETA: 22:32:31 - loss: 0.2931 - acc: 0.874 - ETA: 22:29:13 - loss: 0.2914 - acc: 0.875 - ETA: 22:25:51 - loss: 0.2907 - acc: 0.875 - ETA: 22:22:27 - loss: 0.2894 - acc: 0.876 - ETA: 22:19:16 - loss: 0.2879 - acc: 0.876 - ETA: 22:16:04 - loss: 0.2865 - acc: 0.877 - ETA: 22:12:58 - loss: 0.2846 - acc: 0.878 - ETA: 22:10:20 - loss: 0.2846 - acc: 0.878 - ETA: 22:07:23 - loss: 0.2841 - acc: 0.878 - ETA: 22:04:26 - loss: 0.2825 - acc: 0.879 - ETA: 22:01:38 - loss: 0.2840 - acc: 0.878 - ETA: 21:58:45 - loss: 0.2825 - acc: 0.879 - ETA: 21:55:54 - loss: 0.2816 - acc: 0.879 - ETA: 21:53:15 - loss: 0.2816 - acc: 0.879 - ETA: 21:50:34 - loss: 0.2834 - acc: 0.878 - ETA: 21:47:50 - loss: 0.2818 - acc: 0.879 - ETA: 21:45:10 - loss: 0.2818 - acc: 0.879 - ETA: 21:42:34 - loss: 0.2841 - acc: 0.878 - ETA: 21:39:58 - loss: 0.2850 - acc: 0.877 - ETA: 21:37:25 - loss: 0.2846 - acc: 0.877 - ETA: 21:35:01 - loss: 0.2844 - acc: 0.877 - ETA: 21:32:34 - loss: 0.2842 - acc: 0.877 - ETA: 21:30:11 - loss: 0.2836 - acc: 0.878 - ETA: 21:27:56 - loss: 0.2826 - acc: 0.879 - ETA: 21:25:33 - loss: 0.2840 - acc: 0.878 - ETA: 21:23:13 - loss: 0.2827 - acc: 0.879 - ETA: 21:20:58 - loss: 0.2824 - acc: 0.879 - ETA: 21:18:40 - loss: 0.2828 - acc: 0.879 - ETA: 21:16:26 - loss: 0.2821 - acc: 0.880 - ETA: 21:14:17 - loss: 0.2823 - acc: 0.880 - ETA: 21:12:08 - loss: 0.2826 - acc: 0.880 - ETA: 21:10:10 - loss: 0.2836 - acc: 0.879 - ETA: 21:08:04 - loss: 0.2858 - acc: 0.878 - ETA: 21:05:58 - loss: 0.2872 - acc: 0.877 - ETA: 21:03:56 - loss: 0.2865 - acc: 0.877 - ETA: 21:01:56 - loss: 0.2862 - acc: 0.877 - ETA: 20:59:57 - loss: 0.2872 - acc: 0.877 - ETA: 20:57:56 - loss: 0.2873 - acc: 0.876 - ETA: 20:55:57 - loss: 0.2866 - acc: 0.877 - ETA: 20:53:58 - loss: 0.2856 - acc: 0.877 - ETA: 20:52:07 - loss: 0.2852 - acc: 0.877 - ETA: 20:50:14 - loss: 0.2845 - acc: 0.878 - ETA: 20:48:23 - loss: 0.2847 - acc: 0.878 - ETA: 20:46:32 - loss: 0.2843 - acc: 0.878 - ETA: 20:45:00 - loss: 0.2840 - acc: 0.878 - ETA: 20:44:12 - loss: 0.2842 - acc: 0.878 - ETA: 20:42:40 - loss: 0.2837 - acc: 0.878 - ETA: 20:41:00 - loss: 0.2829 - acc: 0.878 - ETA: 20:39:25 - loss: 0.2822 - acc: 0.878 - ETA: 20:37:49 - loss: 0.2816 - acc: 0.879 - ETA: 20:36:09 - loss: 0.2816 - acc: 0.879 - ETA: 20:34:28 - loss: 0.2811 - acc: 0.879 - ETA: 20:32:47 - loss: 0.2803 - acc: 0.879 - ETA: 20:30:48 - loss: 0.2806 - acc: 0.879 - ETA: 20:28:52 - loss: 0.2798 - acc: 0.879 - ETA: 20:26:58 - loss: 0.2790 - acc: 0.879 - ETA: 20:25:04 - loss: 0.2779 - acc: 0.880 - ETA: 20:23:09 - loss: 0.2778 - acc: 0.880 - ETA: 20:21:17 - loss: 0.2780 - acc: 0.880 - ETA: 20:19:27 - loss: 0.2769 - acc: 0.880 - ETA: 20:17:48 - loss: 0.2763 - acc: 0.881 - ETA: 20:16:43 - loss: 0.2761 - acc: 0.881 - ETA: 20:15:50 - loss: 0.2754 - acc: 0.881 - ETA: 20:15:49 - loss: 0.2755 - acc: 0.881 - ETA: 20:14:55 - loss: 0.2748 - acc: 0.881 - ETA: 20:14:04 - loss: 0.2740 - acc: 0.882 - ETA: 20:13:04 - loss: 0.2731 - acc: 0.882 - ETA: 20:11:33 - loss: 0.2732 - acc: 0.882 - ETA: 20:10:04 - loss: 0.2721 - acc: 0.882 - ETA: 20:08:40 - loss: 0.2711 - acc: 0.883 - ETA: 20:07:14 - loss: 0.2705 - acc: 0.883 - ETA: 20:05:49 - loss: 0.2708 - acc: 0.883 - ETA: 20:04:27 - loss: 0.2701 - acc: 0.883 - ETA: 20:03:05 - loss: 0.2698 - acc: 0.883 - ETA: 20:01:42 - loss: 0.2689 - acc: 0.884 - ETA: 20:00:22 - loss: 0.2681 - acc: 0.884 - ETA: 19:58:59 - loss: 0.2676 - acc: 0.884 - ETA: 19:57:39 - loss: 0.2674 - acc: 0.885 - ETA: 19:56:18 - loss: 0.2664 - acc: 0.885 - ETA: 19:55:00 - loss: 0.2659 - acc: 0.886 - ETA: 19:53:40 - loss: 0.2653 - acc: 0.886 - ETA: 19:52:20 - loss: 0.2647 - acc: 0.886 - ETA: 19:51:05 - loss: 0.2643 - acc: 0.887 - ETA: 19:49:49 - loss: 0.2634 - acc: 0.887 - ETA: 19:48:32 - loss: 0.2627 - acc: 0.887 - ETA: 19:47:49 - loss: 0.2623 - acc: 0.8878\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 371/5219 [=>............................] - ETA: 19:47:21 - loss: 0.2616 - acc: 0.888 - ETA: 19:46:51 - loss: 0.2615 - acc: 0.888 - ETA: 19:45:37 - loss: 0.2609 - acc: 0.888 - ETA: 19:44:24 - loss: 0.2606 - acc: 0.888 - ETA: 19:43:12 - loss: 0.2608 - acc: 0.888 - ETA: 19:41:57 - loss: 0.2608 - acc: 0.888 - ETA: 19:40:45 - loss: 0.2609 - acc: 0.888 - ETA: 19:39:34 - loss: 0.2607 - acc: 0.888 - ETA: 19:38:23 - loss: 0.2615 - acc: 0.888 - ETA: 19:37:13 - loss: 0.2609 - acc: 0.888 - ETA: 19:36:02 - loss: 0.2612 - acc: 0.888 - ETA: 19:34:57 - loss: 0.2614 - acc: 0.888 - ETA: 19:33:52 - loss: 0.2610 - acc: 0.888 - ETA: 19:32:50 - loss: 0.2603 - acc: 0.889 - ETA: 19:31:49 - loss: 0.2605 - acc: 0.888 - ETA: 19:30:43 - loss: 0.2605 - acc: 0.888 - ETA: 19:29:38 - loss: 0.2597 - acc: 0.889 - ETA: 19:28:34 - loss: 0.2600 - acc: 0.888 - ETA: 19:28:00 - loss: 0.2591 - acc: 0.889 - ETA: 19:27:23 - loss: 0.2589 - acc: 0.889 - ETA: 19:26:19 - loss: 0.2587 - acc: 0.889 - ETA: 19:25:22 - loss: 0.2579 - acc: 0.889 - ETA: 19:24:21 - loss: 0.2579 - acc: 0.889 - ETA: 19:23:18 - loss: 0.2574 - acc: 0.889 - ETA: 19:22:16 - loss: 0.2565 - acc: 0.890 - ETA: 19:21:14 - loss: 0.2557 - acc: 0.890 - ETA: 19:20:12 - loss: 0.2558 - acc: 0.890 - ETA: 19:19:11 - loss: 0.2560 - acc: 0.890 - ETA: 19:18:19 - loss: 0.2564 - acc: 0.889 - ETA: 19:17:20 - loss: 0.2558 - acc: 0.890 - ETA: 19:16:28 - loss: 0.2552 - acc: 0.890 - ETA: 19:16:09 - loss: 0.2551 - acc: 0.890 - ETA: 19:15:48 - loss: 0.2551 - acc: 0.890 - ETA: 19:14:49 - loss: 0.2550 - acc: 0.890 - ETA: 19:13:58 - loss: 0.2545 - acc: 0.890 - ETA: 19:13:03 - loss: 0.2540 - acc: 0.890 - ETA: 19:12:08 - loss: 0.2540 - acc: 0.890 - ETA: 19:11:10 - loss: 0.2535 - acc: 0.890 - ETA: 19:10:13 - loss: 0.2546 - acc: 0.890 - ETA: 19:09:17 - loss: 0.2568 - acc: 0.889 - ETA: 19:08:26 - loss: 0.2566 - acc: 0.889 - ETA: 19:07:30 - loss: 0.2578 - acc: 0.888 - ETA: 19:06:37 - loss: 0.2586 - acc: 0.888 - ETA: 19:05:45 - loss: 0.2581 - acc: 0.888 - ETA: 19:05:28 - loss: 0.2576 - acc: 0.889 - ETA: 19:05:11 - loss: 0.2572 - acc: 0.889 - ETA: 19:04:18 - loss: 0.2571 - acc: 0.889 - ETA: 19:03:26 - loss: 0.2598 - acc: 0.888 - ETA: 19:02:34 - loss: 0.2594 - acc: 0.888 - ETA: 19:01:46 - loss: 0.2590 - acc: 0.888 - ETA: 19:01:00 - loss: 0.2592 - acc: 0.888 - ETA: 19:00:09 - loss: 0.2595 - acc: 0.888 - ETA: 18:59:20 - loss: 0.2589 - acc: 0.889 - ETA: 18:58:33 - loss: 0.2582 - acc: 0.889 - ETA: 18:58:17 - loss: 0.2578 - acc: 0.889 - ETA: 18:58:05 - loss: 0.2574 - acc: 0.889 - ETA: 18:57:42 - loss: 0.2568 - acc: 0.890 - ETA: 18:56:50 - loss: 0.2564 - acc: 0.890 - ETA: 18:55:59 - loss: 0.2564 - acc: 0.890 - ETA: 18:55:08 - loss: 0.2562 - acc: 0.890 - ETA: 18:54:21 - loss: 0.2558 - acc: 0.890 - ETA: 18:53:34 - loss: 0.2556 - acc: 0.890 - ETA: 18:52:48 - loss: 0.2553 - acc: 0.890 - ETA: 18:51:59 - loss: 0.2547 - acc: 0.891 - ETA: 18:51:11 - loss: 0.2549 - acc: 0.890 - ETA: 18:50:34 - loss: 0.2544 - acc: 0.891 - ETA: 18:50:19 - loss: 0.2542 - acc: 0.891 - ETA: 18:49:53 - loss: 0.2542 - acc: 0.891 - ETA: 18:49:05 - loss: 0.2553 - acc: 0.890 - ETA: 18:48:17 - loss: 0.2549 - acc: 0.891 - ETA: 18:47:31 - loss: 0.2551 - acc: 0.891 - ETA: 18:46:47 - loss: 0.2555 - acc: 0.891 - ETA: 18:46:01 - loss: 0.2559 - acc: 0.891 - ETA: 18:45:25 - loss: 0.2558 - acc: 0.891 - ETA: 18:45:11 - loss: 0.2563 - acc: 0.890 - ETA: 18:44:58 - loss: 0.2560 - acc: 0.890 - ETA: 75:31:43 - loss: 0.2563 - acc: 0.890 - ETA: 90:53:44 - loss: 0.2568 - acc: 0.890 - ETA: 90:37:53 - loss: 0.2569 - acc: 0.889 - ETA: 90:20:39 - loss: 0.2575 - acc: 0.889 - ETA: 90:03:40 - loss: 0.2574 - acc: 0.889 - ETA: 89:46:42 - loss: 0.2571 - acc: 0.890 - ETA: 89:30:03 - loss: 0.2581 - acc: 0.889 - ETA: 89:13:37 - loss: 0.2595 - acc: 0.889 - ETA: 88:57:05 - loss: 0.2591 - acc: 0.889 - ETA: 88:41:17 - loss: 0.2592 - acc: 0.889 - ETA: 88:26:38 - loss: 0.2595 - acc: 0.889 - ETA: 88:11:13 - loss: 0.2599 - acc: 0.889 - ETA: 87:55:44 - loss: 0.2600 - acc: 0.889 - ETA: 87:40:38 - loss: 0.2595 - acc: 0.889 - ETA: 87:25:59 - loss: 0.2596 - acc: 0.889 - ETA: 87:11:01 - loss: 0.2595 - acc: 0.889 - ETA: 86:55:42 - loss: 0.2595 - acc: 0.889 - ETA: 86:40:55 - loss: 0.2604 - acc: 0.889 - ETA: 86:25:55 - loss: 0.2600 - acc: 0.889 - ETA: 86:11:11 - loss: 0.2596 - acc: 0.889 - ETA: 85:56:27 - loss: 0.2591 - acc: 0.889 - ETA: 85:42:06 - loss: 0.2587 - acc: 0.889 - ETA: 85:27:39 - loss: 0.2587 - acc: 0.889 - ETA: 85:12:48 - loss: 0.2586 - acc: 0.889 - ETA: 84:58:04 - loss: 0.2582 - acc: 0.890 - ETA: 84:43:28 - loss: 0.2581 - acc: 0.889 - ETA: 84:28:53 - loss: 0.2578 - acc: 0.890 - ETA: 84:14:28 - loss: 0.2575 - acc: 0.890 - ETA: 84:00:06 - loss: 0.2571 - acc: 0.890 - ETA: 83:45:56 - loss: 0.2567 - acc: 0.890 - ETA: 83:32:01 - loss: 0.2566 - acc: 0.890 - ETA: 83:18:14 - loss: 0.2567 - acc: 0.890 - ETA: 83:04:47 - loss: 0.2568 - acc: 0.890 - ETA: 82:51:23 - loss: 0.2563 - acc: 0.890 - ETA: 82:37:36 - loss: 0.2561 - acc: 0.890 - ETA: 82:23:56 - loss: 0.2569 - acc: 0.890 - ETA: 82:10:21 - loss: 0.2567 - acc: 0.890 - ETA: 81:56:49 - loss: 0.2564 - acc: 0.890 - ETA: 81:43:20 - loss: 0.2564 - acc: 0.890 - ETA: 81:30:00 - loss: 0.2560 - acc: 0.890 - ETA: 81:16:49 - loss: 0.2561 - acc: 0.890 - ETA: 81:03:51 - loss: 0.2556 - acc: 0.891 - ETA: 80:50:46 - loss: 0.2558 - acc: 0.890 - ETA: 80:37:48 - loss: 0.2555 - acc: 0.890 - ETA: 80:24:55 - loss: 0.2551 - acc: 0.891 - ETA: 80:12:17 - loss: 0.2549 - acc: 0.891 - ETA: 79:59:33 - loss: 0.2546 - acc: 0.891 - ETA: 79:47:06 - loss: 0.2556 - acc: 0.891 - ETA: 79:34:30 - loss: 0.2555 - acc: 0.891 - ETA: 79:22:13 - loss: 0.2552 - acc: 0.891 - ETA: 79:09:59 - loss: 0.2548 - acc: 0.891 - ETA: 78:58:05 - loss: 0.2549 - acc: 0.891 - ETA: 78:46:03 - loss: 0.2549 - acc: 0.891 - ETA: 78:33:49 - loss: 0.2546 - acc: 0.891 - ETA: 78:21:31 - loss: 0.2540 - acc: 0.892 - ETA: 78:09:15 - loss: 0.2537 - acc: 0.892 - ETA: 77:57:14 - loss: 0.2535 - acc: 0.892 - ETA: 77:45:13 - loss: 0.2531 - acc: 0.892 - ETA: 77:33:11 - loss: 0.2526 - acc: 0.892 - ETA: 77:21:11 - loss: 0.2522 - acc: 0.892 - ETA: 77:09:16 - loss: 0.2519 - acc: 0.893 - ETA: 76:57:25 - loss: 0.2516 - acc: 0.893 - ETA: 76:45:38 - loss: 0.2515 - acc: 0.893 - ETA: 76:33:55 - loss: 0.2515 - acc: 0.893 - ETA: 76:22:47 - loss: 0.2510 - acc: 0.893 - ETA: 76:11:44 - loss: 0.2509 - acc: 0.893 - ETA: 76:00:46 - loss: 0.2507 - acc: 0.893 - ETA: 75:49:54 - loss: 0.2505 - acc: 0.893 - ETA: 75:38:45 - loss: 0.2502 - acc: 0.893 - ETA: 75:27:41 - loss: 0.2501 - acc: 0.893 - ETA: 75:17:07 - loss: 0.2497 - acc: 0.893 - ETA: 75:06:07 - loss: 0.2495 - acc: 0.894 - ETA: 74:55:10 - loss: 0.2496 - acc: 0.893 - ETA: 74:44:17 - loss: 0.2491 - acc: 0.894 - ETA: 74:33:28 - loss: 0.2486 - acc: 0.894 - ETA: 74:22:43 - loss: 0.2487 - acc: 0.894 - ETA: 74:12:14 - loss: 0.2484 - acc: 0.894 - ETA: 74:01:42 - loss: 0.2482 - acc: 0.894 - ETA: 73:51:12 - loss: 0.2479 - acc: 0.894 - ETA: 73:40:39 - loss: 0.2478 - acc: 0.894 - ETA: 73:30:12 - loss: 0.2475 - acc: 0.894 - ETA: 73:19:47 - loss: 0.2473 - acc: 0.894 - ETA: 73:09:26 - loss: 0.2474 - acc: 0.894 - ETA: 72:59:07 - loss: 0.2470 - acc: 0.895 - ETA: 72:48:53 - loss: 0.2464 - acc: 0.895 - ETA: 72:38:43 - loss: 0.2465 - acc: 0.895 - ETA: 72:28:36 - loss: 0.2460 - acc: 0.895 - ETA: 72:18:33 - loss: 0.2456 - acc: 0.895 - ETA: 72:08:38 - loss: 0.2451 - acc: 0.895 - ETA: 71:58:44 - loss: 0.2453 - acc: 0.896 - ETA: 71:48:55 - loss: 0.2453 - acc: 0.896 - ETA: 71:39:07 - loss: 0.2454 - acc: 0.896 - ETA: 71:29:46 - loss: 0.2452 - acc: 0.896 - ETA: 71:20:28 - loss: 0.2447 - acc: 0.896 - ETA: 71:10:54 - loss: 0.2451 - acc: 0.896 - ETA: 71:01:21 - loss: 0.2449 - acc: 0.896 - ETA: 70:52:01 - loss: 0.2447 - acc: 0.896 - ETA: 70:42:42 - loss: 0.2443 - acc: 0.896 - ETA: 70:33:33 - loss: 0.2442 - acc: 0.896 - ETA: 70:24:06 - loss: 0.2443 - acc: 0.896 - ETA: 70:14:43 - loss: 0.2445 - acc: 0.896 - ETA: 70:05:26 - loss: 0.2449 - acc: 0.896 - ETA: 69:56:20 - loss: 0.2450 - acc: 0.895 - ETA: 69:47:20 - loss: 0.2448 - acc: 0.895 - ETA: 69:38:30 - loss: 0.2451 - acc: 0.895 - ETA: 69:29:29 - loss: 0.2448 - acc: 0.895 - ETA: 69:20:25 - loss: 0.2451 - acc: 0.895 - ETA: 69:11:39 - loss: 0.2447 - acc: 0.895 - ETA: 69:02:58 - loss: 0.2448 - acc: 0.895 - ETA: 68:54:23 - loss: 0.2450 - acc: 0.8959"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 459/5219 [=>............................] - ETA: 68:45:44 - loss: 0.2448 - acc: 0.895 - ETA: 68:37:03 - loss: 0.2447 - acc: 0.895 - ETA: 68:28:43 - loss: 0.2442 - acc: 0.896 - ETA: 68:20:42 - loss: 0.2440 - acc: 0.896 - ETA: 68:12:17 - loss: 0.2435 - acc: 0.896 - ETA: 68:03:40 - loss: 0.2431 - acc: 0.896 - ETA: 67:55:05 - loss: 0.2431 - acc: 0.896 - ETA: 67:46:37 - loss: 0.2426 - acc: 0.896 - ETA: 67:38:17 - loss: 0.2425 - acc: 0.897 - ETA: 67:30:00 - loss: 0.2423 - acc: 0.897 - ETA: 67:21:59 - loss: 0.2422 - acc: 0.897 - ETA: 67:13:46 - loss: 0.2425 - acc: 0.896 - ETA: 67:05:32 - loss: 0.2428 - acc: 0.896 - ETA: 66:57:16 - loss: 0.2426 - acc: 0.897 - ETA: 66:49:02 - loss: 0.2424 - acc: 0.897 - ETA: 66:40:53 - loss: 0.2424 - acc: 0.897 - ETA: 66:32:46 - loss: 0.2420 - acc: 0.897 - ETA: 66:24:39 - loss: 0.2418 - acc: 0.897 - ETA: 66:16:45 - loss: 0.2417 - acc: 0.897 - ETA: 66:08:53 - loss: 0.2417 - acc: 0.897 - ETA: 66:01:01 - loss: 0.2416 - acc: 0.897 - ETA: 65:53:25 - loss: 0.2413 - acc: 0.897 - ETA: 65:45:47 - loss: 0.2414 - acc: 0.897 - ETA: 65:38:05 - loss: 0.2412 - acc: 0.897 - ETA: 65:30:35 - loss: 0.2413 - acc: 0.897 - ETA: 65:23:02 - loss: 0.2419 - acc: 0.897 - ETA: 65:15:33 - loss: 0.2417 - acc: 0.897 - ETA: 65:08:00 - loss: 0.2413 - acc: 0.897 - ETA: 65:00:28 - loss: 0.2409 - acc: 0.898 - ETA: 64:52:47 - loss: 0.2406 - acc: 0.898 - ETA: 64:45:13 - loss: 0.2404 - acc: 0.897 - ETA: 64:37:32 - loss: 0.2410 - acc: 0.897 - ETA: 64:29:58 - loss: 0.2407 - acc: 0.898 - ETA: 64:22:39 - loss: 0.2406 - acc: 0.897 - ETA: 64:15:30 - loss: 0.2403 - acc: 0.897 - ETA: 64:08:19 - loss: 0.2399 - acc: 0.898 - ETA: 64:01:05 - loss: 0.2397 - acc: 0.898 - ETA: 63:53:52 - loss: 0.2393 - acc: 0.898 - ETA: 63:46:41 - loss: 0.2392 - acc: 0.898 - ETA: 63:39:38 - loss: 0.2395 - acc: 0.898 - ETA: 63:32:27 - loss: 0.2394 - acc: 0.898 - ETA: 63:25:13 - loss: 0.2391 - acc: 0.898 - ETA: 63:18:02 - loss: 0.2392 - acc: 0.898 - ETA: 63:10:52 - loss: 0.2390 - acc: 0.898 - ETA: 63:03:43 - loss: 0.2386 - acc: 0.898 - ETA: 62:56:35 - loss: 0.2381 - acc: 0.899 - ETA: 62:49:29 - loss: 0.2380 - acc: 0.899 - ETA: 62:42:26 - loss: 0.2378 - acc: 0.898 - ETA: 62:35:25 - loss: 0.2378 - acc: 0.899 - ETA: 62:28:25 - loss: 0.2377 - acc: 0.899 - ETA: 62:21:28 - loss: 0.2376 - acc: 0.899 - ETA: 62:14:33 - loss: 0.2376 - acc: 0.899 - ETA: 62:07:39 - loss: 0.2377 - acc: 0.899 - ETA: 62:00:46 - loss: 0.2378 - acc: 0.899 - ETA: 61:53:55 - loss: 0.2377 - acc: 0.899 - ETA: 61:47:08 - loss: 0.2377 - acc: 0.899 - ETA: 61:40:21 - loss: 0.2374 - acc: 0.899 - ETA: 61:33:37 - loss: 0.2372 - acc: 0.899 - ETA: 61:26:55 - loss: 0.2372 - acc: 0.899 - ETA: 61:20:15 - loss: 0.2370 - acc: 0.899 - ETA: 61:13:37 - loss: 0.2371 - acc: 0.899 - ETA: 61:07:01 - loss: 0.2369 - acc: 0.899 - ETA: 61:00:28 - loss: 0.2368 - acc: 0.899 - ETA: 60:53:59 - loss: 0.2365 - acc: 0.899 - ETA: 60:47:30 - loss: 0.2362 - acc: 0.899 - ETA: 60:41:00 - loss: 0.2361 - acc: 0.899 - ETA: 60:34:31 - loss: 0.2368 - acc: 0.899 - ETA: 60:28:05 - loss: 0.2368 - acc: 0.899 - ETA: 60:21:40 - loss: 0.2364 - acc: 0.899 - ETA: 60:15:17 - loss: 0.2360 - acc: 0.899 - ETA: 60:08:54 - loss: 0.2362 - acc: 0.899 - ETA: 60:02:33 - loss: 0.2363 - acc: 0.899 - ETA: 59:56:16 - loss: 0.2361 - acc: 0.899 - ETA: 59:50:09 - loss: 0.2359 - acc: 0.899 - ETA: 59:44:19 - loss: 0.2356 - acc: 0.899 - ETA: 59:38:18 - loss: 0.2353 - acc: 0.900 - ETA: 59:32:11 - loss: 0.2350 - acc: 0.900 - ETA: 59:26:00 - loss: 0.2347 - acc: 0.900 - ETA: 59:19:51 - loss: 0.2346 - acc: 0.900 - ETA: 59:13:45 - loss: 0.2345 - acc: 0.900 - ETA: 59:07:40 - loss: 0.2343 - acc: 0.900 - ETA: 59:01:39 - loss: 0.2342 - acc: 0.900 - ETA: 58:55:42 - loss: 0.2339 - acc: 0.900 - ETA: 58:49:43 - loss: 0.2338 - acc: 0.900 - ETA: 58:44:10 - loss: 0.2341 - acc: 0.900 - ETA: 58:38:20 - loss: 0.2339 - acc: 0.900 - ETA: 58:32:25 - loss: 0.2340 - acc: 0.900 - ETA: 58:27:26 - loss: 0.2338 - acc: 0.9007"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-107-e91d978aabb0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_validate_samples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     class_weight ='auto')\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1658\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1659\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2977\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2978\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2979\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2980\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2981\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2936\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2937\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2938\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1458\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Fit the transfer learning model\n",
    "history_transfer_learning = model.fit_generator(\n",
    "    train_generator, #training data\n",
    "    epochs=NB_EPOCHS,\n",
    "    steps_per_epoch =num_trian_samples,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps = num_validate_samples,\n",
    "    class_weight ='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
